{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with steps\n",
    "\n",
    "This notebook shows how to **create** steps, **fit** them to data, **transform** new data and take advantage of persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from steps.base import Step, BaseTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import a dataset from scikit-learn for our experiments and divide it into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "num_train = int(0.8 * len(y_digits))\n",
    "\n",
    "sample_ids = np.random.permutation(len(y_digits))\n",
    "\n",
    "train_ids = sample_ids[:num_train]\n",
    "test_ids = sample_ids[num_train:]\n",
    "\n",
    "print('{} samples for training'.format(len(train_ids)))\n",
    "print('{} samples for test'.format(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps communicate data between each other with plain **Python dictionaries**. This makes it easy to pass collections of **arbitrary data types** (Numpy arrays, Pandas dataframes, etc.). The basic structure is as follows (you can get much more fancy but we leave that to the next example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {'input':\n",
    "                {\n",
    "                     'X': X_digits[train_ids, :],\n",
    "                     'y': y_digits[train_ids],\n",
    "                }\n",
    "            }\n",
    "\n",
    "data_test = {'input':\n",
    "                {\n",
    "                     'X': X_digits[test_ids, :],\n",
    "                     'y': y_digits[test_ids],\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating steps\n",
    "Let's create a simple step - first we define an **adapter** to tell it how to interpret its input data dictionary (this allows you to do lots of clever things but we'll stick to basics for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adapter just extracts the values under keys 'X' and 'y' from the node 'input'\n",
    "input_adapter = {\n",
    "                     'X': [('input', 'X')],\n",
    "                     'y': [('input', 'y')]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second ingredient of a step is a transformer, which is where the real action happens. You just have to define a class following a **simple API** and then it's up to you to be as creative as you want!\n",
    "\n",
    "... or you can just **wrap you favorite Scikit-learn estimator** like we do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class RandomForestTransformer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        self.estimator = RandomForestClassifier()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        y_pred  = self.estimator.predict(X)\n",
    "        return {'y_pred': y_pred}  # TODO: exaplain this\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump(self.estimator, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        self.estimator = joblib.load(filepath)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does the transformer do? It must be able to:\n",
    "* **initialize** itself\n",
    "* **fit** and **transform** the incoming data prepared by the adapter; when transforming, the result should be returned as a **dictionary** that can be **passed on to the next step**\n",
    "* **save** and **load** its parameters; this is handy when you're trying to avoid re-computing things over and over.\n",
    "\n",
    "See how flexible this is? You can just as easily wrap your Keras or Pytorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine our adapter and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_step = Step(name='classifier',\n",
    "                       transformer=RandomForestTransformer(),\n",
    "                       input_data=['input'],                 \n",
    "                       adapter=input_adapter,\n",
    "                       cache_dirpath='./cache'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's our one-step pipeline finished. You can visualize it too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just about the simplest pipeline you can imagine. Now let's train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_step.clean_cache()\n",
    "preds_train = classifier_step.fit_transform(data_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = np.sum(preds_train['y_pred'] == data_train['input']['y']) / data_train['input']['y'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running test data through our pipeline is as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = classifier_step.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = np.sum(preds_test['y_pred'] == data_test['input']['y']) / data_test['input']['y'].size\n",
    "print('Test accuracy = {:.4f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some predictions to see if they're sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(4, 8, figsize=(10, 6))\n",
    "for ii, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(data_test['input']['X'][ii].reshape(8, 8), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('pred = {}'.format(preds_test['y_pred'][ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's about it for a start! Have a look at the next notebook for a more advanced example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
